{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "weird-harvard",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "## SQLite\n",
    "### CSV datasets\n",
    "CSV datasets have been imported as SQLite table using PyCharm. Importing a CSV dataset rows that have malformed data are automatically dropped, this is time efficient because no script is needed.\n",
    "\n",
    "### JSON datasets\n",
    "Unfortunately PyCharm does not provide a direct import for JSON files. Only one dataset (crypto_telegram_groups) is in json, so a dedicated scprit ([json2db.py](./json2db.py)) has been wrote to import it in the database.\n",
    "\n",
    "## Data tweaks\n",
    "### Twitter\n",
    "Twitter datasets contain some data about november 2020, but that are removed to avoid data interpolation in the charts.\n",
    "\n",
    "### Datetime re-formatting\n",
    "Datetimes in the datasets have been re-formatted to a common format 'Year-Month-Day Hours:Minutes:Seconds'. This process has been done by [datetimeformatter.py](./datetimeformatter.py). The process is simply:\n",
    "- database lines are read one by one\n",
    "- if the raw datetime matches listed patterns, it is converted using them. In case of no matches, dateutil package is used to recognize pattern and get formatted datetime\n",
    "- database line is updated using formatted datetime\n",
    "- 'date' column is populated removing time from formatted datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-worst",
   "metadata": {},
   "source": [
    "# News crawling\n",
    "\n",
    "Some news in the final dataset have been crawled from google news using 'Google News' from Hurin Hu. For each day:\n",
    "- 'Google News' scraps first news page\n",
    "- 'newspaper' library recover all informations about each article\n",
    "- the results are saved in a csv\n",
    "\n",
    "The complete code can be found in [googlecrawler.py](./googlecrawler.py).\n",
    "\n",
    "Note: the complete csv is next imported in a SQLite to be merged with others news datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-dominant",
   "metadata": {},
   "source": [
    "# Text classification\n",
    "\n",
    "Texts have been classified Flair pre-trained english classifier.\n",
    "Due to time constraints the fast-classifier has been choosed.\n",
    "Standard classifier is 10 times slower than the fast one (more or less) and with 26 million of tweeter this difference means 240 hours vs 24 hours.\n",
    "\n",
    "Fast-classifier is a RNN that uses Word2Vec for words embedding.\n",
    "\n",
    "Each text is classified by [flairclassifier.py](./flairclassifier.py) returning a label and a confidence.\n",
    "\n",
    "### Label\n",
    "Label che assume one of three values:\n",
    "- POSITIVE\n",
    "- NEUTRAL\n",
    "- NEGATIVE\n",
    "\n",
    "NEUTRAL has a double meaning:\n",
    "- neutral text\n",
    "- failed classification\n",
    "\n",
    "### Confidence\n",
    "Confidence is a value in [0, 1] and rappresents the polarity of sentiment and the reliability of the classification.\n",
    "\n",
    "### Signed score\n",
    "Sentiment and confidence could be condensed in one number between -1 and 1, left half for a negative classification and right half for a positive one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-authentication",
   "metadata": {},
   "source": [
    "# Datasets merging\n",
    "\n",
    "To merge datasets about the same media (like, all datasets about twitter)two scripts are been used:\n",
    "\n",
    "## Generic merger\n",
    "[csvmerger.py](./csvmerger.py) is the first script that has been wrote to merge database tables. It works but it needs to be re-wrote, in orign it was wrote to merge csvs and than it war adapted to merge databases, so there is code junk and stufs like that. It was used to merge twitter's and telegram's datasets.\n",
    "\n",
    "## News merger\n",
    "[newsmerger.py](./newsmerger.py) has been used to merge news datasets; it's a re-wrote version of csvmerger.py but specific for the news; the priciples are the same, but has fewer code junk and it's easier to read.\n",
    "\n",
    "## How datasets have been merged\n",
    "Given a list of database's table and a target table the script:\n",
    "- read line by line each table\n",
    "- if a line is already in the target table, the scipt \"join\" the lines trying to complete missing values\n",
    "- else the script inserts the table's line in the targe one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-parent",
   "metadata": {},
   "source": [
    "# Tweets words count\n",
    "[tweetwordscounter.py](./tweetwordscounter.py) is the scruipt that counts how many words there is a tweet.\n",
    "This value is inserted in the database for later use, for now, it's just used to calculate average tweets length of each of the users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-armenia",
   "metadata": {},
   "source": [
    "# Common colors helper\n",
    "[palette.py](./palette.py) acts like a wrapper of a dictionary.\n",
    "Importing palette dictionary inside of palette.py the same color palette can be used in all notebooks, without using HEX values; colors can be simply used accessing the dictiory with keys like 'positive', 'negative', 'price', 'twitter', 'smooth_news', 'strong_telegram' and so on.\n",
    "\n",
    "An important upgrade could be the creation of anothe palette colorblind-safe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-sudan",
   "metadata": {},
   "source": [
    "# Data grouping\n",
    "\n",
    "## Daily info\n",
    "Grouping by day is made using SQL:\n",
    "- all data in a table is grouped by date and label with a query\n",
    "- for each group, grouping columns are also selected as part of new lines\n",
    "- each line is corredated with the group lines count and average confidence\n",
    "\n",
    "`\n",
    "SELECT date, label, COUNT(ROWID) as count, AVG(conf) as conf FROM tweet GROUP BY date, label;\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-richardson",
   "metadata": {},
   "source": [
    "## Users retweets\n",
    "Also the csv about retweets for each user was made using SQL:\n",
    "- entries are grouped by username\n",
    "- new entries have: username, full_name, sum of retweets\n",
    "- new entries are ordered in descending order\n",
    "\n",
    ">SELECT username, full_name,  SUM(retweets) as retweets FROM tweet GROUP BY username ORDER BY retweets DESC;\n",
    "\n",
    "\n",
    "## Users tweets\n",
    "As for retweets, csv about tweets per user was made using SQL:\n",
    "- entries are grouped by username\n",
    "- new entries have: username, full_name, count of unique tweets\n",
    "- new entries are ordered in descending order\n",
    "\n",
    ">SELECT username, full_name,  COUNT(DISTINCT text) as tweets FROM tweet GROUP BY username ORDER BY tweets DESC;\n",
    "\n",
    "\n",
    "## Average user's tweets length\n",
    "As before, in these case SQL is a friend:\n",
    "- entries are grouped by username\n",
    "- new entries have: username, full_name, average words count\n",
    "- new entries are ordered in descending order\n",
    "\n",
    ">SELECT username, full_name, AVG(words) as words_avg FROM tweet GROUP BY username ORDER BY words_avg DESC;\n",
    "\n",
    "## Grouped data to csv\n",
    "To write a csv containing the grouped data; each query has been used to create a view and then export it to csv usind PyCharm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
